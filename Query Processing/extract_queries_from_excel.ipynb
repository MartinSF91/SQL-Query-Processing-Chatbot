{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "from functools import reduce, partial\n",
    "from sql_metadata import Parser\n",
    "import sqlparse\n",
    "import re\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 10:47:01,580 - INFO - Started extract_queries_and_meta_data.\n",
      "2025-04-02 10:47:01,594 - INFO - Started read_in_excel_files from path ./1_migrated_excel_queries/test.xlsx.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 10:47:01,954 - INFO - Successfully read 1 excel file(s) from ./1_migrated_excel_queries/test.xlsx.\n",
      "2025-04-02 10:47:01,956 - INFO - Started process_query_df.\n",
      "2025-04-02 10:47:01,958 - INFO - Started preprocess_query_df for migrated queries.\n",
      "2025-04-02 10:47:03,123 - INFO - Preprocessing of migrated queries successful.\n",
      "2025-04-02 10:47:03,123 - INFO - Started write_queries_to_json for migrated queries.\n",
      "2025-04-02 10:47:03,149 - INFO - Queries successfully written to JSON-file.\n",
      "2025-04-02 10:47:03,151 - INFO - Started extract_query_meta_data from migrated queries.\n",
      "2025-04-02 10:47:03,154 - INFO - Meta data from migrated queries successfully extracted.\n",
      "2025-04-02 10:47:03,155 - INFO - Started extract_query_tables from migrated queries.\n",
      "2025-04-02 10:47:03,652 - INFO - Tables from migrated queries successfully extracted.\n",
      "2025-04-02 10:47:03,652 - INFO - Started extract_query_columns from migrated queries.\n",
      "2025-04-02 10:47:04,135 - INFO - Columns from migrated queries successfully extracted.\n",
      "2025-04-02 10:47:04,136 - INFO - Started write_query_data_to_json for migrated queries.\n",
      "2025-04-02 10:47:04,140 - INFO - 13 queries successfully written to JSON-file. Path: ./4_json_results/migrated_query_data.json\n",
      "2025-04-02 10:47:04,141 - INFO - Completed process_query_df for migrated queries.\n"
     ]
    }
   ],
   "source": [
    "extract_queries_and_meta_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `extract_queries_and_meta_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_queries_and_meta_data():\n",
    "    logging.info(f\"Started {extract_queries_and_meta_data.__name__}.\")\n",
    "\n",
    "    # Migrated Queries\n",
    "    MIGRATED_QUERY_SOURCE_PATH = \"./1_migrated_excel_queries/test.xlsx\"\n",
    "    MIGRATED_QUERY_TARGET_PATH = \"./3_extracted_queries/migrated_queries\"\n",
    "    MIGRATED_QUERY_IDENTIFIER = \"migrated\"\n",
    "\n",
    "    migrated_query_df = read_in_excel_files(MIGRATED_QUERY_SOURCE_PATH)\n",
    "    process_query_df(migrated_query_df, MIGRATED_QUERY_TARGET_PATH, MIGRATED_QUERY_IDENTIFIER)\n",
    "\n",
    "    # New Queries\n",
    "    # NEW_QUERY_SOURCE_PATH = \"./2_new_excel_queries/*.xlsx\"\n",
    "    # NEW_QUERY_TARGET_PATH = \"./3_extracted_queries/new_queries\"\n",
    "    # NEW_QUERY_IDENTIFIER = \"new\"\n",
    "\n",
    "    # new_query_df = read_in_excel_files(NEW_QUERY_SOURCE_PATH)\n",
    "    # process_query_df(new_query_df, NEW_QUERY_TARGET_PATH, NEW_QUERY_IDENTIFIER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `read_in_excel_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_excel_files(PATH):\n",
    "    logging.info(f\"Started {read_in_excel_files.__name__} from path {PATH}.\")\n",
    "    try:\n",
    "        excel_data = glob.glob(PATH)\n",
    "\n",
    "        dataframes = [pd.read_excel(data, engine=\"openpyxl\") for data in excel_data]\n",
    "        query_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "        logging.info(f\"Successfully read {len(excel_data)} excel file(s) from {PATH}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in reading excel files: {e}\")\n",
    "\n",
    "    return query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `process_query_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_df(query_df, QUERY_TARGET_PATH, QUERY_IDENTIFIER):\n",
    "    logging.info(f\"Started {process_query_df.__name__}.\")\n",
    "\n",
    "    query_df_processed = reduce(\n",
    "        lambda accu, func: func(accu),\n",
    "        [\n",
    "            partial(\n",
    "                preprocess_query_df,\n",
    "                QUERY_IDENTIFIER=QUERY_IDENTIFIER\n",
    "            ),\n",
    "            partial(\n",
    "                write_queries_to_json,\n",
    "                QUERY_TARGET_PATH=QUERY_TARGET_PATH,\n",
    "                QUERY_IDENTIFIER=QUERY_IDENTIFIER\n",
    "            ),\n",
    "        ],\n",
    "        query_df\n",
    "    )\n",
    "\n",
    "    query_meta_data = extract_query_meta_data(query_df_processed, QUERY_IDENTIFIER)\n",
    "\n",
    "    query_tables = extract_query_tables(query_df_processed, QUERY_IDENTIFIER)\n",
    "\n",
    "    query_columns = extract_query_columns(query_df_processed, QUERY_IDENTIFIER)\n",
    "\n",
    "    write_query_data_to_json(query_meta_data, query_tables, query_columns, QUERY_IDENTIFIER)\n",
    "\n",
    "    logging.info(f\"Completed {process_query_df.__name__} for {QUERY_IDENTIFIER} queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `preprocess_query_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query_df(query_df, QUERY_IDENTIFIER):\n",
    "    logging.info(f\"Started {preprocess_query_df.__name__} for {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    query_df = query_df.dropna(subset=[\"SQL\"]).fillna(\"\")\n",
    "\n",
    "    query_df[\"SQL\"] = (query_df[\"SQL\"]\n",
    "                       .str.replace('ê', 'e').str.replace('é', 'e').str.replace('è', 'e').str.replace('à', 'a').str.replace('ç', 'c')\n",
    "                       .str.replace('ô', 'o').str.replace('û', 'u').str.replace('ù', 'u').str.replace('î', 'i').str.replace('ï', 'i')\n",
    "                       .str.replace('â', 'a').str.replace('ä', 'a').str.replace('ö', 'o').str.replace('ü', 'u').str.replace('ÿ', 'y')\n",
    "                       .str.replace('ñ', 'n').str.replace('É', 'E').str.replace('È', 'E').str.replace('À', 'A').str.replace('Ç', 'C')\n",
    "                       .str.replace('Ô', 'O').str.replace('Û', 'U').str.replace('Ù', 'U').str.replace('Î', 'I').str.replace('Ï', 'I')\n",
    "                       .str.replace('Â', 'A').str.replace('Ä', 'A').str.replace('Ö', 'O').str.replace('Ü', 'U').str.replace('Ÿ', 'Y')\n",
    "                       .str.replace('Ñ', 'N')\n",
    "    )\n",
    "\n",
    "    pattern_1 = r'WITH\\s+\"\\w+\"\\s+AS\\s*\\(.*?\\)\\s*SELECT'\n",
    "    pattern_2 = r'\"[^\"]*\"\\.'\n",
    "\n",
    "    for index, query in enumerate(query_df[\"SQL\"]):\n",
    "        formatted_query = query.upper()\n",
    "        formatted_query = sqlparse.format(formatted_query, reindent=True, keyword_case='upper', strip_comments=True).strip()\n",
    "        formatted_query = re.sub(pattern_1, 'SELECT', formatted_query, flags=re.DOTALL)\n",
    "        formatted_query = re.sub(pattern_2, '', formatted_query)\n",
    "        query_df.at[index, 'SQL'] = formatted_query\n",
    "\n",
    "    logging.info(f\"Preprocessing of {QUERY_IDENTIFIER} queries successful.\")\n",
    "\n",
    "    return query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `write_queries_to_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_queries_to_json(query_df, QUERY_TARGET_PATH, QUERY_IDENTIFIER):\n",
    "    logging.info(f\"Started {write_queries_to_json.__name__} for {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    for index, query in enumerate(query_df[\"SQL\"]):\n",
    "        try:\n",
    "            with open(f\"{QUERY_TARGET_PATH}/{QUERY_IDENTIFIER}_query_{index}.sql\", \"w\", encoding='utf-8') as file:\n",
    "                file.write(query)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing query {index} to JSON-file: {e}\")\n",
    "\n",
    "    logging.info(f\"Queries successfully written to JSON-file.\")\n",
    "\n",
    "    return query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `extract_query_meta_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_meta_data(query_df, QUERY_IDENTIFIER):\n",
    "    logging.info(f\"Started {extract_query_meta_data.__name__} from {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    meta_data = {}\n",
    "    for index, (idx, row) in enumerate(query_df.iterrows()):\n",
    "        if \"Datasource\" not in row:\n",
    "            datasource = \"nA\"\n",
    "        else:\n",
    "            datasource = row[\"Datasource\"].upper()\n",
    "        if \"Product Name\" not in row:\n",
    "            product_name = \"nA\"\n",
    "        else:\n",
    "            product_name = row[\"Product Name\"].upper()\n",
    "        if \"Report Name\" not in row:\n",
    "            report_name = \"nA\"\n",
    "        else:\n",
    "            report_name = row[\"Report Name\"].upper()\n",
    "        if \"Report Path\" not in row:\n",
    "            report_path = \"nA\"\n",
    "        else:\n",
    "            report_path = row[\"Report Path\"].upper()\n",
    "\n",
    "        meta_data[f\"report_{index}\"] = {\n",
    "                f\"report_name\": report_name,\n",
    "                f\"product_name\": product_name,\n",
    "                f\"report_path\": report_path,\n",
    "                f\"datasource\": datasource,\n",
    "            }\n",
    "\n",
    "    logging.info(f\"Meta data from {QUERY_IDENTIFIER} queries successfully extracted.\")\n",
    "\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `extract_query_tables`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_tables(query_df, QUERY_IDENTIFIER):\n",
    "    logging.info(f\"Started {extract_query_tables.__name__} from {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    INVALID_TABLE_NAMES = [\n",
    "        \"BOTH\",\n",
    "        \"TRIM\",\n",
    "        \"SUBSTR\",\n",
    "        \"SUM\",\n",
    "        \"CAST\",\n",
    "        \"CASE\",\n",
    "        \"TRUNC\",\n",
    "        \"CONCAT\",\n",
    "        \"TO_CHAR\",\n",
    "        \"ROUND\",\n",
    "        \"DISTINCT\",\n",
    "        \"TRAILING\",\n",
    "        \"LENGTH\",\n",
    "        \"BOTH\"\n",
    "    ]\n",
    "\n",
    "    extracted_tables = []\n",
    "\n",
    "    for index, query in query_df[\"SQL\"].items():\n",
    "        try:\n",
    "            raw_query_tables = []\n",
    "            raw_query_tables_cleansed = []\n",
    "\n",
    "            parser = Parser(query)\n",
    "            raw_query_tables = parser.tables\n",
    "            raw_query_tables = [table.upper() for table in raw_query_tables if table.upper() not in INVALID_TABLE_NAMES]\n",
    "            raw_query_tables_cleansed = [table.split(\".\")[-1] for table in raw_query_tables]\n",
    "\n",
    "            raw_query_tables.sort()\n",
    "            raw_query_tables_cleansed.sort()\n",
    "\n",
    "            extracted_tables.append([raw_query_tables, raw_query_tables_cleansed])\n",
    "\n",
    "            # logging.info(f\"Extracted tables from query at index {index}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting table {query}: {e}\")\n",
    "\n",
    "    logging.info(f\"Tables from {QUERY_IDENTIFIER} queries successfully extracted.\")\n",
    "\n",
    "    return extracted_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `extract_query_columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_columns(query_df, QUERY_IDENTIFIER):\n",
    "    logging.info(f\"Started {extract_query_columns.__name__} from {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    extracted_columns = []\n",
    "    error_queries = []\n",
    "\n",
    "    for index, query in query_df[\"SQL\"].items():\n",
    "        try:\n",
    "            processed_query_columns = []\n",
    "            processed_query_columns_cleansed = []\n",
    "\n",
    "            parser = Parser(query)\n",
    "            processed_query_columns = parser.columns\n",
    "            processed_query_columns_cleansed = [col.split(\".\")[-1] for col in processed_query_columns]\n",
    "\n",
    "            processed_query_columns = sorted(set(processed_query_columns))\n",
    "            processed_query_columns_cleansed = sorted(set(processed_query_columns_cleansed))\n",
    "\n",
    "            extracted_columns.append([processed_query_columns, processed_query_columns_cleansed])\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting columns in query at index {index}: {e}.\")\n",
    "            error_queries.append({'query_identifier': QUERY_IDENTIFIER, 'index': index, 'query': query, 'error': str(e)})\n",
    "\n",
    "    logging.info(f\"Columns from {QUERY_IDENTIFIER} queries successfully extracted.\")\n",
    "\n",
    "    return extracted_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `write_query_data_to_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_data_to_json(meta_data, tables, columns, QUERY_IDENTIFIER):\n",
    "    logging.info(f\"Started {write_query_data_to_json.__name__} for {QUERY_IDENTIFIER} queries.\")\n",
    "\n",
    "    JSON_PATH = f\"./4_json_results/{QUERY_IDENTIFIER}_query_data.json\"\n",
    "\n",
    "    for index, (table, column) in enumerate(zip(tables, columns)):\n",
    "        meta_data[f\"report_{index}\"][f\"tables\"] = table[0]\n",
    "        meta_data[f\"report_{index}\"][f\"tables_cleansed\"] = table[1]\n",
    "        meta_data[f\"report_{index}\"][f\"columns\"] = column[0]\n",
    "        meta_data[f\"report_{index}\"][f\"columns_cleansed\"] = column[1]\n",
    "\n",
    "    with open(JSON_PATH, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(meta_data, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "    logging.info(f\"{len(meta_data.keys())} queries successfully written to JSON-file. Path: {JSON_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
